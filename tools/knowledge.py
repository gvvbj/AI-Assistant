import os
import hashlib
import chromadb
from chromadb.api.types import Documents, EmbeddingFunction, Embeddings
from utils.logger import logger
from utils.error_handling import safe_execute
from tools.registry import tool_registry
import ollama
import re

# === å¯é€‰ä¾èµ–å¯¼å…¥ ===
try:
    import pypdf
except ImportError:
    pypdf = None

try:
    import docx
except ImportError:
    docx = None

try:
    from flashrank import Ranker, RerankRequest
    HAS_FLASHRANK = True
except ImportError:
    HAS_FLASHRANK = False
    logger.warning("FlashRank not installed. Rerank disabled.")

class OllamaEmbeddingFunction(EmbeddingFunction):
    def __init__(self, model_name="nomic-embed-text", base_url="http://127.0.0.1:11434"):
        self.model_name = model_name
        self.client = ollama.Client(host=base_url)

    def __call__(self, input: Documents) -> Embeddings:
        embeddings = []
        for text in input:
            try:
                resp = self.client.embeddings(model=self.model_name, prompt=text)
                embeddings.append(resp["embedding"])
            except Exception as e:
                logger.error(f"Embedding Error: {e}")
                embeddings.append([0.0]*768)
        return embeddings

class KnowledgeBase:
    _client = None
    _collection = None
    _current_embed_model = None
    _ranker = None
    _current_ranker_model = None

    def __init__(self):
        self.db_path = "chroma_db"
        os.makedirs(self.db_path, exist_ok=True)
        try:
            self._client = chromadb.PersistentClient(path=self.db_path)
        except Exception as e:
            logger.error(f"Chroma Init Fail: {e}")

    def _get_collection(self, embed_model_name):
        if not self._client: return None
        if self._collection is None or self._current_embed_model != embed_model_name:
            self._current_embed_model = embed_model_name
            try:
                safe_name = f"kb_{embed_model_name.replace(':', '_').replace('.', '_')}"
                self._collection = self._client.get_or_create_collection(
                    name=safe_name,
                    embedding_function=OllamaEmbeddingFunction(model_name=embed_model_name)
                )
            except Exception as e:
                logger.error(f"Collection Error: {e}")
                return None
        return self._collection

    def _get_ranker(self, model_name):
        if not HAS_FLASHRANK: return None
        real_name = model_name.split(" ")[0]
        if self._ranker is None or self._current_ranker_model != real_name:
            try:
                self._ranker = Ranker(model_name=real_name, cache_dir="models")
                self._current_ranker_model = real_name
            except Exception as e:
                logger.error(f"Reranker Init Fail: {e}")
                return None
        return self._ranker

    def _calculate_hash(self, file_path):
        h = hashlib.md5()
        with open(file_path, 'rb') as f: h.update(f.read())
        return h.hexdigest()

    def _extract_text(self, file_path):
        """æå–æ–‡æœ¬ï¼šæ”¯æŒ txt, md, xlsx, pdf, docx"""
        ext = os.path.splitext(file_path)[1].lower()
        
        # 1. Excel
        if ext in ['.xlsx', '.xls']:
            import openpyxl
            wb = openpyxl.load_workbook(file_path, data_only=True)
            text = []
            for sheet in wb.worksheets:
                text.append(f"--- Sheet: {sheet.title} ---")
                for row in sheet.iter_rows(values_only=True):
                    cleaned_row = [str(c) for c in row if c is not None]
                    if cleaned_row:
                        row_txt = " | ".join(cleaned_row)
                        text.append(row_txt)
            return "\n".join(text)
        
        # 2. PDF
        elif ext == '.pdf':
            if not pypdf: return "Error: ç¼ºå°‘ pypdf åº“ï¼Œæ— æ³•è§£æ PDF"
            try:
                reader = pypdf.PdfReader(file_path)
                text = []
                for page in reader.pages:
                    text.append(page.extract_text() or "")
                return "\n".join(text)
            except Exception as e:
                return f"PDFè§£æå¤±è´¥: {e}"

        # 3. Word (Docx)
        elif ext == '.docx':
            if not docx: return "Error: ç¼ºå°‘ python-docx åº“ï¼Œæ— æ³•è§£æ Docx"
            try:
                doc = docx.Document(file_path)
                return "\n".join([p.text for p in doc.paragraphs])
            except Exception as e:
                return f"Docxè§£æå¤±è´¥: {e}"
        
        # 4. å›¾ç‰‡ (ä¸æ”¯æŒ)
        elif ext in ['.png', '.jpg', '.jpeg', '.bmp']:
            return None # è¿”å› None è¡¨ç¤ºä¸æ”¯æŒï¼Œä¸Šå±‚ä¼šå¤„ç†
            
        # 5. çº¯æ–‡æœ¬
        elif ext in ['.txt', '.md', '.py', '.json', '.csv', '.html']:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: return f.read()
            except: return None
            
        return None

    # === å®‰å…¨çš„è¿­ä»£åˆ‡åˆ†ç®—æ³• (ä¿®å¤å†…å­˜æº¢å‡º/å´©æºƒ) ===
    def _safe_split_text(self, text, chunk_size=600, overlap=100):
        if not text: return []
        
        chunks = []
        total_len = len(text)
        start = 0
        
        while start < total_len:
            # ç¡®å®šç¡¬æˆªæ­¢ç‚¹
            end = min(start + chunk_size, total_len)
            
            # å¦‚æœè¿˜æ²¡åˆ°æ–‡ä»¶æœ«å°¾ï¼Œå°è¯•ä¼˜åŒ–åˆ‡åˆ†ç‚¹ï¼ˆæ‰¾æ¢è¡Œç¬¦ï¼‰
            if end < total_len:
                # åœ¨çª—å£ååŠéƒ¨åˆ†å¯»æ‰¾æœ€è¿‘çš„æ¢è¡Œç¬¦
                # æœç´¢èŒƒå›´ï¼š[end - chunk_size//2, end]
                lookback_limit = max(start, end - chunk_size // 2)
                
                # ä¼˜å…ˆæ‰¾åŒæ¢è¡Œï¼ˆæ®µè½ï¼‰
                last_newline = text.rfind('\n\n', lookback_limit, end)
                if last_newline != -1:
                    end = last_newline + 2 # ä¿ç•™æ¢è¡Œç¬¦
                else:
                    # å…¶æ¬¡æ‰¾å•æ¢è¡Œ
                    last_newline = text.rfind('\n', lookback_limit, end)
                    if last_newline != -1:
                        end = last_newline + 1
                    else:
                        # å†å…¶æ¬¡æ‰¾å¥å·
                        last_period = text.rfind('ã€‚', lookback_limit, end)
                        if last_period != -1:
                            end = last_period + 1
                        
                        # å®åœ¨æ‰¾ä¸åˆ°åˆ†éš”ç¬¦ï¼Œå°±ç¡¬åˆ‡ï¼Œä¸å›é€€ï¼Œé˜²æ­¢æ­»å¾ªç¯
            
            # æå–åˆ‡ç‰‡
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # è®¡ç®—ä¸‹ä¸€ä¸ª start
            # æ­£å¸¸æ­¥è¿›æ˜¯ chunké•¿åº¦ - overlap
            # ä½†è¦é˜²æ­¢æ­»å¾ªç¯ï¼ˆæ­¥è¿›ä¸º0ï¼‰ï¼Œå¼ºåˆ¶è‡³å°‘å‰è¿› 1
            step = max(1, (end - start) - overlap)
            
            # å¦‚æœæ˜¯ç¡¬åˆ‡ä¸”åˆ°äº†æœ«å°¾ï¼Œç›´æ¥é€€å‡º
            if end == total_len:
                break
                
            start += step
            
        return chunks

    @safe_execute("æ–‡æ¡£ç´¢å¼•å¤±è´¥")
    def add_document(self, file_path, embed_model_name="nomic-embed-text"):
        coll = self._get_collection(embed_model_name)
        if not coll: return "DBè¿æ¥å¤±è´¥"
        
        fname = os.path.basename(file_path)
        fhash = self._calculate_hash(file_path)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = coll.get(where={"file_hash": fhash})
        if existing['ids']:
            return f"æ–‡ä»¶ {fname} å·²å­˜åœ¨"
            
        content = self._extract_text(file_path)
        
        if content is None:
            # ç‰¹æ®Šå¤„ç†å›¾ç‰‡ç­‰ä¸æ”¯æŒæ ¼å¼
            ext = os.path.splitext(file_path)[1].lower()
            if ext in ['.png', '.jpg', '.jpeg']:
                return "âŒ å›¾ç‰‡æ–‡ä»¶ä¸æ”¯æŒæ–‡æœ¬ç´¢å¼•ã€‚è¯·ä½¿ç”¨'ä»£ç è§£é‡Šå™¨'æˆ– Vision æ¨¡å‹è¿›è¡Œåˆ†æã€‚"
            return f"âŒ æ ¼å¼ {ext} ä¸æ”¯æŒæ–‡æœ¬è§£æ"
            
        if not content.strip(): 
            return "æ–‡ä»¶å†…å®¹ä¸ºç©º"
        
        # ä½¿ç”¨æ–°çš„å®‰å…¨åˆ‡åˆ†ç­–ç•¥
        chunks = self._safe_split_text(content, chunk_size=600, overlap=100)
        
        if not chunks: return "æœªèƒ½ç”Ÿæˆæœ‰æ•ˆåˆ‡ç‰‡"

        ids = [f"{fhash}_{i}" for i in range(len(chunks))]
        metas = [{"source": fname, "file_hash": fhash} for _ in chunks]
        
        # æ‰¹é‡æ·»åŠ ï¼Œé˜²æ­¢å•æ¬¡è¯·æ±‚è¿‡å¤§
        batch_size = 100
        for i in range(0, len(chunks), batch_size):
            coll.add(
                documents=chunks[i:i+batch_size], 
                ids=ids[i:i+batch_size], 
                metadatas=metas[i:i+batch_size]
            )
            
        return f"ç´¢å¼•æˆåŠŸï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªåˆ‡ç‰‡"

    @tool_registry.register(
        name="kb_search",
        description="Search the external Knowledge Base. Use this tool WHENEVER the user asks for information, facts, documents, or details that might be stored in the database.",
        parameters={
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "Keywords to search for"}
            },
            "required": ["query"]
        }
    )
    def search(self, query, embed_model_name="nomic-embed-text", rerank_model_name=None):
        coll = self._get_collection(embed_model_name)
        if not coll: return "DB Error"
        
        top_k = 15 if rerank_model_name else 5
        
        try:
            res = coll.query(query_texts=[query], n_results=top_k)
            docs = res['documents'][0]
            metas = res['metadatas'][0]
            
            if not docs: return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"
            
            final_res = []
            
            if rerank_model_name and HAS_FLASHRANK:
                ranker = self._get_ranker(rerank_model_name)
                if ranker:
                    passages = [{"id": str(i), "text": d, "meta": m} for i, (d, m) in enumerate(zip(docs, metas))]
                    rerank_req = RerankRequest(query=query, passages=passages)
                    ranked_res = ranker.rerank(rerank_req)
                    
                    for item in ranked_res[:5]:
                        src = item['meta'].get('source', 'unknown')
                        final_res.append(f"ğŸ“„ [Source: {src}]\n{item['text']}")
                    return "\n\n".join(final_res)

            for i in range(min(5, len(docs))):
                src = metas[i].get('source', 'unknown')
                final_res.append(f"ğŸ“„ [Source: {src}]\n{docs[i]}")
            return "\n\n".join(final_res)
            
        except Exception as e:
            return f"æ£€ç´¢å¼‚å¸¸: {e}"

    def get_files(self, embed_model_name="nomic-embed-text"):
        coll = self._get_collection(embed_model_name)
        if not coll: return []
        try:
            data = coll.get(include=['metadatas'])
            return list(set([m['source'] for m in data['metadatas'] if m]))
        except: return []

    def delete_file(self, fname, embed_model_name="nomic-embed-text"):
        coll = self._get_collection(embed_model_name)
        if coll: coll.delete(where={"source": fname})

knowledge_tool = KnowledgeBase()